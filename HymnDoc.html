<!DOCTYPE HTML>
<html>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<head>
		<title>Hymnal App</title>
		<link rel="stylesheet" type="text/css" href="docmain.css">
	</head>
	<body>
    <section id="content">
        <nav>
            <ul>
                <li id="logo">the hymnal project</li>
                <li class="menuitem">home</li>
                <li class="menuitem">hymns</li>
                <li class="menuitem">projects</li>
            </ul>
            <br><br>
        </nav>
		<pre><h2>a brute-force, copyright-infringing saga</h2></pre>
		<h2>STEP 1: scrape hymns from hymnal.net for the lyrics</h2>
			<p>Originally, the files from http://hymnal.net consisted of simple web pages containing the desired content, the lyrics, in <code>&lt;ol&gt;</code> and <code>&lt;li&gt;</code> tags.* <br>
				For example, here&apos;s the HTML of interest from Hymn 100. You can see the current state of the site at <a href="http://www.hymnal.net/hymn.php/h/100" target="_blank">http://www.hymnal.net/hymn.php/h/100</a>.
			</p>
			<pre><div class="code">
<span class ="source">[Source: http://www.hymnal.net/hymn.php/h/100]</span>
...
&lt;div id="lyrics"&gt;
&lt;ol&gt;&lt;li value="1"&gt;The Maker of the universe&lt;br/&gt;As Man, for man was made a curse.&lt;br/&gt;The claims of law which He had made,&lt;br/&gt;Unto the uttermost He paid.&lt;/li&gt;&lt;li value="2"&gt;His holy fingers made the bough
...
			</div></pre>
			<p>	Grabbing the content of these pages was straightforward, once I knew roughly what the structure of the website was. I controlled the downloading of songs with a simple loop that kept track of which number it had reached, resetting the URL every time, and sticking the number at the end of the URL each time another song was downloaded.
			</p>
			<pre><div class="code">
<span class="source">[Source: parse.py]</span>
...
from urllib.request import urlopen
web_page = urlopen(url)
  file = open(filename, 'w')
  parser.feed(web_page.read().decode('utf-8'))
...
			</div></pre>
			<p>Behind the scenes, I had two classes that did much of the downloading work. The point was that I didn’t want all of the extra stuff in the HTML page, only the lyrics, as shown above. I began with the default package with which Python 3 ships, importing the <code>HTMLParser</code> class from html.parser and overriding most of its functions.
			</p>
			<pre><div class="code" id="code">
<span class="source">[Source: site1.py]</span>
from html.parser import HTMLParser

class HymnHTMLParser(HTMLParser):
    def __init__(self, tagging, want_tag, start_on=None):
        HTMLParser.__init__(self)
        self.recording = 0
        self.data = []
        self.want_tag, self.tagging, self.start_on = want_tag, tagging, start_on
        
    def handle_starttag(self, tag, attribute):
        if self.start_on:
            if tag == self.start_on[0]:
                for name, value in attribute:
                    if name == self.start_on[1] and value == self.start_on[2]:
                        self.tagging = True
        if self.tagging:
            if tag in self.want_tag:
                for name, value in attribute:
                    self.data.append(str(value) + ': ')
                self.recording += 1

...

    def handle_entityref(self, name):
        if self.tagging:
            if name == 'mdash':
                self.data.append(str('-'))
            elif name != 'nbsp':
                logging.info(name)

...

    def handle_data(self, data):
        if self.recording:
            self.data.append(data)
			</div></pre>
			<p>Obviously, there were a lot of details in this implementation that I would rather not have kept track of, including errant HTML characters, entities, and the dreaded conversion from bytes to strings, with all of the problems with encoding. I thought my needs were basic enough that I could handle this project with this simple class, but when I ran into errors with broken HTML, I finally made the switch to <a href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a>. This switch simplified a lot of things, including handling of the entities and malformed HTML, so I could focus on grabbing the structure and content of the document.
			</p>
			<p>An aside: why did I need the second site parser? Well, obviously I hadn’t made my <code>HymnHTMLParser</code> very extensible, and though I made some misguided attempts to do so (see the <code>want_tag</code> and <code>start_on</code> variables I introduced in an effort to make my code reusable), they were not going to cut it for any other web pages, as I soon realized.
			</p>
			<p>Once I saw that many of the hymns on http://hymnal.net did not actually contain the lyrics, but instead placeholder text and a link to an external site, I realized I’d need another parser for these other sites. I made simple, hacky check for these links by checking if <code>line[:4] == 'http':</code> each time I got a line from the song. Then, I switched control of parsing to the function in the other class, by calling <code>site2.parse_html(line, file)</code> and changing a flag, <code>parsed</code>, that let the original parser know that the other function had taken care of all the parsing.
			</p>
			<pre><div class="code">
<span class="source">[Source: site2.py]</span>
from bs4 import BeautifulSoup
from urllib.request import urlopen
from string import punctuation
import util

def parse_url(url):
    soup = BeautifulSoup(urlopen(url).read().decode('utf-8'))
    def filt(tag):
        return (tag.name == ‘font’ and
                tag.has_key(‘class’) and
                tag.parent.parent.parent.name == ‘table’ and
                tag.parent.parent.parent.has_key(‘width’) and
                (tag.parent.parent.parent[‘width’] in [‘400’, ‘500’] or
                 tag.parent.parent.parent[‘width’] == ‘760’ and tag[‘class’] == [‘text2’]))
        
    soup = BeautifulSoup(str(soup.find_all(filt)))
    return util.replace_unicode(soup.text)
...
			</div></pre>
			<p>That simplified the code for parsing the second site from which I needed lyrics, though it’s clear from my giant filter statement that it still involved quite a bit of staring at the patterns in the pages to figure out what content I should scrape. I also didn’t know how to handle the Unicode characters that cropped up in my soup, so I just looked up their equivalents (thanks, <a href="http://webdesign.about.com/od/localization/l/blhtmlcodes-ascii.htm">about.com</a>, for once) and replaced them.</p>
			<pre><div class="code">
<span class="source">[Source: util.py]</span>
def replace_unicode(s):
    unicode_chars = {'\xa0': " ", 
                      '\x91': "'", 
                      '\x92': "'", 
                      '\x20\x20': "\\n", 
                      '\x20': " ", 
                      '\x93': '\"', 
                      '\x94': '\"', 
                      ",,": "", 
                      '\n, ': '\\n', 
                      ' Chorus\n,\n,': 'chorus:'}
    for char in unicode_chars:
        s = s.replace(char, unicode_chars[char])
    return s
			</div></pre>
		<h2>STEP 2: turning the content into application-consumable json</h2>
			<p>I wanted my application to have as lightweight a footprint on the network as possible, and JSON seemed like the ideal choice. I didn’t put much research into using the built-in module for this in Python 3, resulting in a headache of string-parsing that left me an expert at parsing strings in Python 3 (as wonderful an experience as parsing strings can be because of Python’s useful library of functions) without regular expressions (which I had and have not yet mastered) but used a lot of time that could have been avoided.
			</p>
			<p>By the way, JSON took me a little while to learn, since I wasn’t familiar with Javascript before this project. I wasn’t clear that there are two different types: 1) JSON objects, and 2) JSON arrays. There are good libraries for dealing with both, and they’re both highly compatible with other languages, including Java, as I found out later. 
			</p>
			<p>The basic format for a JSON Object looks like this:</p>
			<pre><div class="code">
{ “Toyota”: “Prius”, “Honda”: “Accord”, “Hyundai”: “Sonata”,
... }
				</div></pre>
			<p>The basic format for a JSON Array looks like this:</p>
			<pre><div class="code">
[ “Ferrari”, “Porsche”, “Lamborghini”, “Corvette”, 
... ]
			</div></pre>
			<p>And the two can be nested within each other, to create more complex structures, like XML, though I didn’t need that for this app. The important difference between objects and arrays, which I didn’t take into account before diving into string parsing, is that JSON arrays are ordered, while JSON objects make no guarantee about which components will be retrieved first.</p>
			<p>I also had to learn how file I/O works in Python, and luckily, this was just as easy. Python will assume you are starting from the directory where your file is if you don’t have an absolute path, so I created a hymns folder that would hold the hymns I was currently downloading. Then, when I was writing my files, I just had to use some simple syntax:</p>
			<pre><div class="code">
file = open(filename, 'w')
file.write(song_text)

# alternatively, you can open a file just for reading.
file = open(filename, 'r')
song_text = file.read()
			</div></pre>
			<p>At this point, also, I became tired of waiting for every song to download once again when I ran my downloader. I added a function, in <code>util.py</code> that got all <code>.txt</code> files in the same directory and checked if they were already downloaded before going through the whole ordeal each time.</p>
			<pre><div class="code">
<span class="source">[Source: util.py]</span>
def get_files_in_directory(directory):
    import os
    dirList = os.listdir(directory)
    files = []
    for d in dirList:
        if d[len(d) - 3: len(d)] == 'txt':
            files.append(d)
    return files

[Source: run.py]
filenames = util.get_files_in_directory('./hymns') 

def run(song_type, start_song, end_song):

...

        # open file and replace symbols
        if filename not in filenames:
            parse.parse(url, filename, parser)
			</div></pre>
			<p>In the end, with two different sites that I had to turn into a uniform JSON format, I resorted to a pretty reliable, but low-level method. I looped through every letter in all 1,885 songs, adding formatting where I needed it. </p>
			<p>First, I set up a container string for my resulting JSON. I went through the original string, letter by letter. I checked the letters and words around it to see if I needed to add any special characters to the resulting string. Whether I did or didn’t, I would add the letter from the original string into the resulting string, preserving all of its content.</p>
			<p>I began my attempt by outputting some JSON-object formatted songs. </p>
        </section>
	</body>
</html>